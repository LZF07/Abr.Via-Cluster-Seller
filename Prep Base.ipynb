{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(spark.sparkContext, 'be63ac50-4f9d-43f3-8f26-ad6572f65430', 'p-7bda336230f76ada26590bc6c4038045546d3287')\npc = project.project_context", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210531012322-0024\nKERNEL_ID = d99b1e73-5674-48f7-9a3d-d089a64a120e\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Importando algumas bibliotecas\nimport sys\nimport shutil\nimport types\nimport pandas as pd\npd.options.display.max_columns = 999\n\nfrom botocore.client import Config\nimport ibm_boto3\n\nfrom datetime import datetime\nfrom datetime import datetime, timedelta", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "credentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-8c64e377-89e3-424a-b706-72ea9ab5fd59',\n    'IBM_API_KEY_ID': 'vNrUHeW9-R1HdOM2ld5PDpR96H0VdqryNzcmyVlNFne_',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'projetoviagrupo1-donotdelete-pr-q8ymlbqhihqlvv',\n    'CES' : 'compraentregastatus.csv',\n    'SL' : 'skulojista.csv',\n    'CE' : 'compraentrega.csv',\n    'C' : 'compra.csv',\n    'CESKU' : 'compraentregasku.csv',\n    'CFPG' : 'compraformapagamento.csv',\n    'LOJ' : 'lojista.csv',\n    'CAT': 'skucategoria.csv',\n    'NPS' : 'visaolojistavendanps.csv',\n    'FPG' : 'formapagamento.csv'\n}", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cgsClient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id = credentials_1['IBM_API_KEY_ID'],\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "path = '/home/spark/shared/tmp/'\ntry:\n    os.makedirs(path)\nexcept OSError:\n    shutil.rmtree(path)\n    print (\"Deletado diret\u00f3rio: %s\" % path)\n    print (\"Diret\u00f3rio criado com sucesso: %s\" % path)\n    os.makedirs(path)\nelse:\n    print (\"Diret\u00f3rio criado com sucesso: %s\" % path)", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Deletado diret\u00f3rio: /home/spark/shared/tmp/\nDiret\u00f3rio criado com sucesso: /home/spark/shared/tmp/\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "def upload_file_cos(cos, local_file_name, credentials,key):  \n    \n    try:\n        res=cos.upload_file(Filename=local_file_name+key, Bucket=credentials['BUCKET'],Key=key)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(key+' - File Uploaded -'+ (datetime.now() - timedelta(hours=3)).strftime(\"%d/%m/%Y %H:%M:%S\"))", "execution_count": 6, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "def download_file_cos(cos, local_file_name, credential): \n    \n    for i in credential:\n        if list(credential).index(i) > 4:\n\n            '''\n            Wrapper function to download a file from cloud object storage using the\n            credential dict provided and loading it into memory\n            '''\n            try:\n                res=cos.download_file(Bucket=credential['BUCKET'], Key=credential[i], Filename=local_file_name +credential[i])\n            except Exception as e:\n                print(credential[i] + ' - Exception', e)\n            else:\n                print(credential[i] + ' - File Downloaded')\n    \n    print(\"Done. -\"+(datetime.now() - timedelta(hours=3)).strftime(\"%d/%m/%Y %H:%M:%S\"))", "execution_count": 7, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "download_file_cos(cgsClient, path, credentials_1)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "compraentregastatus.csv - File Downloaded\nskulojista.csv - File Downloaded\ncompraentrega.csv - File Downloaded\ncompra.csv - File Downloaded\ncompraentregasku.csv - File Downloaded\ncompraformapagamento.csv - File Downloaded\nlojista.csv - File Downloaded\nskucategoria.csv - File Downloaded\nvisaolojistavendanps.csv - File Downloaded\nformapagamento.csv - File Downloaded\nDone. -30/05/2021 22:24:11\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<h1>ETL</h1>"}, {"metadata": {}, "cell_type": "code", "source": "spark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/compraentregastatus.csv\").createOrReplaceTempView(\"compraentregastatuslog\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/skulojista.csv\").createOrReplaceTempView(\"skulojista\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/compraentrega.csv\").createOrReplaceTempView(\"compraentrega\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/compra.csv\").createOrReplaceTempView(\"compra\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/compraentregasku.csv\").createOrReplaceTempView(\"compraentregasku\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/compraformapagamento.csv\").createOrReplaceTempView(\"compraformapagamento\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/lojista.csv\").createOrReplaceTempView(\"lojista\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/skucategoria.csv\").createOrReplaceTempView(\"categoria\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/visaolojistavendanps.csv\").createOrReplaceTempView(\"visaolojistavendanps\")\nspark.read.option(\"header\",'true').csv(\"/home/spark/shared/tmp/formapagamento.csv\").createOrReplaceTempView(\"formapagamento\")", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pgto = spark.sql('''\nSELECT \nIDCOMPRA,\nIDBANDEIRA,\n\nBoleto,\nCartao_Presente,\nCredito,\nDebito,\nOutro as Outro_Meio_Pagamento,\nVale,\nTotal_Pagamentos_Insucesso,\nTotal_Pagamentos_Sucesso\n\nFROM\n(SELECT \nROW_NUMBER() OVER (\n    PARTITION BY IDCOMPRA, IDBANDEIRA\n    ORDER BY idcompraformapagamentostatus ASC\n) as LINHA,\nIDCOMPRA,\nIDBANDEIRA,\nidcompraformapagamentostatus,\n\nCOALESCE(Boleto,0) AS Boleto,\nCOALESCE(`Cart\u00e3o Presente`,0) AS Cartao_Presente,\nCOALESCE(Credito,0) AS Credito,\nCOALESCE(Debito,0) AS Debito,\nCOALESCE(Outro,0) AS Outro,\nCOALESCE(Vale,0) AS Vale,\n\nCOALESCE(Total_Pagamentos_Insucesso,0) AS Total_Pagamentos_Insucesso,\nCOALESCE(Total_Pagamentos_Sucesso,0) AS Total_Pagamentos_Sucesso\n\nFROM (\nselect\nCE.IDCOMPRA,\nCE.IDBANDEIRA,\nCASE \n  WHEN FPG.nome = 'Pix' AND FPG.tipo = 'QrCode' THEN FPG.nome\n  WHEN FPG.tipo in ('Cartao de Debito', 'Loja Debito') THEN 'Debito'\n  WHEN FPG.tipo = 'Loja Credito' or FPG.tipo = 'Credito' THEN 'Credito'\n  WHEN FPG.tipo IN ('Boleto Prazo B2B','Boleto B2B') THEN 'Boleto B2B'\n  WHEN FPG.tipo IN ('Boleto', 'Carne', 'Cart\u00e3o Presente', 'Credito', 'Debito', 'Loja Dinheiro','QrCode', 'Vale') THEN FPG.tipo\n  ELSE 'Outro'\n  END AS Tipo_Pagamento,\nCFPG.idcompraformapagamentostatus,\ncount(DISTINCT CFPG.idcompraformapagamento) as Quantidade_Tentativas_Pagamentos,\n\nsum(count(distinct (case when CFPG.idcompraformapagamentostatus != 2 then CFPG.idcompraformapagamento else null end))) OVER (\n    PARTITION BY CE.IDCOMPRA, CE.IDBANDEIRA \n    ORDER BY CE.IDCOMPRA ASC\n) as Total_Pagamentos_Insucesso,\n\nsum(count(distinct (case when CFPG.idcompraformapagamentostatus = 2 then CFPG.idcompraformapagamento else null end))) OVER (\n    PARTITION BY CE.IDCOMPRA, CE.IDBANDEIRA \n    ORDER BY CE.IDCOMPRA ASC\n) as Total_Pagamentos_Sucesso\n\nfrom compraentrega CE\n\nLEFT JOIN compraformapagamento CFPG\non CE.IDCOMPRA = CFPG.idcompra and CE.IDBANDEIRA = CFPG.IDBANDEIRA\n\nLEFT JOIN FORMAPAGAMENTO FPG\nON CFPG.IDFORMAPAGAMENTO = FPG.IDFORMAPAGAMENTO AND CFPG.IDBANDEIRA = FPG.IDBANDEIRA\n\nwhere CE.origem = 'LJ'\ngroup by 1,2,3,4)\nPIVOT (\nsum(Quantidade_Tentativas_Pagamentos)\nFOR Tipo_Pagamento in ('Boleto', 'Cart\u00e3o Presente', 'Credito', 'Debito', 'Outro', 'Vale')))\nWHERE LINHA = 1\n''')\npgto.createOrReplaceTempView('pivot_pgto')", "execution_count": 10, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "carrinho = spark.sql('''select \nROW_NUMBER() OVER (\n    PARTITION BY ce.idcompra, ce.IDBANDEIRA \n    ORDER BY coalesce(ROUND((CESKU.valorvendaunidade + CESKU.valorfretecomdesconto),2),val.Valor+val.Frete) desc\n) as Item_Carrinho,\n\nce.idcompra,\nce.IDBANDEIRA,\nc.flagaprovado,\nC.idcanalvenda,\nce.idfreteentregatipo,\nce.idcompraentrega,\nce.idcompraentregastatus as status,\ndev.Devolucao,\n\nc.Parceiro,\nc.Campanha,\n  \nCE.idlojista,\nce.flagenviadomarketplace,\n\nc.data,\ndate_format(C.data, 'HH:mm:ss') as Hora_Pedido,\ndate(C.data) as Dia_Pedido,\ndate_format(C.data,'yyyy-MM') as Mes_pedido,\n\nCASE\n    WHEN date(ce.dataentrega) > date(ce.dataprevisao) THEN 1\n    ELSE 0\n    END AS Flag_Atraso,\n    \nDATEDIFF(date(ce.dataprevisao),date(ce.dataentrega)) as Tempo_atraso,\n\nce.dataentrega,\nce.dataprevisao,\n\nCESKU.idsku,\ncoalesce(ROUND(CESKU.valorvendaunidade,2),val.Valor) as Valor,\ncoalesce(ROUND(CESKU.valorfretecomdesconto,2),val.Frete) as Frete,\ncoalesce(ROUND((CESKU.valorvendaunidade + CESKU.valorfretecomdesconto),2),val.Valor+val.Frete) AS Valor_Com_Frete,\n\nCAT.idsetor AS IDCATEGORIA,\nCAT.nomesetor AS CATEGORIA,\nCAT.iddepartamento AS IDDEPARTAMENTO,\nCAT.nomedepartamento AS DEPARTAMENTO,\n\npg.Boleto,\npg.Cartao_Presente,\npg.Credito,\npg.Debito,\npg.Outro_Meio_Pagamento,\npg.Vale,\npg.Total_Pagamentos_Insucesso,\npg.Total_Pagamentos_Sucesso\n\nfrom compraentrega CE\n\nLEFT JOIN compra C\nON CE.IDCOMPRA = C.IDCOMPRA AND CE.IDBANDEIRA = C.IDBANDEIRA\n\nLEFT JOIN (select\nidcompra,\nidbandeira,\nmax(case when origem = 'TD' then 1 else 0 end) as Devolucao\nfrom compraentrega\ngroup by 1,2) DEV\nON CE.IDCOMPRA = DEV.IDCOMPRA AND CE.IDBANDEIRA = DEV.IDBANDEIRA\n\nLEFT JOIN lojista LJ\nON CE.idlojista = LJ.idlojista AND CE.IDBANDEIRA = LJ.IDBANDEIRA\n\nLEFT JOIN compraentregasku CESKU\non CESKU.IDBANDEIRA = CE.IDBANDEIRA AND CESKU.idcompraentrega = ce.idcompraentrega AND CESKU.idcompraentregaskupai IS NULL\n\nLEFT JOIN categoria CAT\non CESKU.idsku = CAT.idsku\n\nLEFT JOIN pivot_pgto pg\non ce.idcompra = pg.idcompra and ce.IDBANDEIRA = pg.IDBANDEIRA\n\nLEFT JOIN (select \nidsku,\nidbandeira,\nround(avg(valorvendaunidade),2) as Valor,\nround(avg(valorfretecomdesconto),2) as Frete\n\nfrom compraentregasku\nwhere idcompraentregaskupai IS NULL\ngroup by 1,2) val\non CESKU.idsku = val.idsku and CESKU.idbandeira = val.idbandeira\n\nWHERE CE.idfreteentregatipo NOT IN ('17','19','20')\nAND CE.origem = 'LJ'\n''')\ncarrinho.createOrReplaceTempView('carrinho')", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "carrinho.toPandas().to_csv('/home/spark/shared/tmp/carrinho.csv', index=False)\nupload_file_cos(cgsClient, path,credentials_1,'carrinho.csv')", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "carrinho.csv - File Uploaded -30/05/2021 22:29:48\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "preco_lojista = spark.sql('''\nselect \nLJ.idlojista,\nLJ.idbandeira,\nSKL.idsku,\nCAT.idsetor AS IDCATEGORIA,\nCAT.nomesetor AS CATEGORIA,\nCAT.iddepartamento AS IDDEPARTAMENTO,\nCAT.nomedepartamento AS DEPARTAMENTO,\nSKL.precoanterior,\nSKL.precovenda AS precoatual,\nSKL.dataultimaatualizacao as DataAtualizacao\n\nfrom lojista LJ\n\nLEFT JOIN skulojista SKL\nON LJ.idlojista = SKL.idlojista AND LJ.idbandeira = SKL.idbandeira\n\nLEFT JOIN CATEGORIA CAT\nON SKL.idsku = CAT.idsku\n''')", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "preco_lojista.toPandas().to_csv('/home/spark/shared/tmp/preco_lojista.csv', index=False)\nupload_file_cos(cgsClient, path,credentials_1,'preco_lojista.csv')", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "preco_lojista.csv - File Uploaded -30/05/2021 22:31:12\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "preco_lojista.toPandas().to_json('/home/spark/shared/tmp/preco_lojista.json',orient=\"records\")\nupload_file_cos(cgsClient, path,credentials_1,'preco_lojista.json')", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "preco_lojista.json - File Uploaded -30/05/2021 22:31:44\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "dia_nps = spark.sql('''\nselect \nyear(DtAprovacaoPedidoSite) as ano,\nmonth(DtAprovacaoPedidoSite) as mes,\nday(DtAprovacaoPedidoSite) as dia,\nidlojista,\nidbandeira AS Bandeira,\nNmClusterLojista as ClusterGMV,\nsum(QtResposta) as QtResposta,\nsum(QtRespostaDetratora) as QtRespostaDetratora,\nsum(QtRespostaPromotora) as QtRespostaPromotora,\n((sum(QtRespostaPromotora) - sum(QtRespostaDetratora)) / sum(QtResposta)) as `%NPS`\n\nfrom visaolojistavendanps\ngroup by 1,2,3,4,5,6\n''')\ndia_nps.createOrReplaceTempView('dia_nps')", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dia_nps.toPandas().to_csv('/home/spark/shared/tmp/dia_nps.csv', index=False)\nupload_file_cos(cgsClient, path,credentials_1,'dia_nps.csv')", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "dia_nps.csv - File Uploaded -30/05/2021 22:31:51\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "compilado_vendas = spark.sql('''\nselect\nT1.*,\ncoalesce(nps.QtResposta,0) as QtResposta,\ncoalesce(nps.QtRespostaDetratora,0) as QtRespostaDetratora,\ncoalesce(nps.QtRespostaPromotora,0) as QtRespostaPromotora,\ncoalesce(nps.`%NPS`,0) as `%NPS`\n\nfrom\n(select\nyear(dia_pedido) as ano,\nmonth(dia_pedido) as mes,\nday(dia_pedido) as dia,\nidlojista,\nIDBANDEIRA,\nidsku,\nIDCATEGORIA,\nCATEGORIA,\nIDDEPARTAMENTO,\nDEPARTAMENTO,\nsum(Valor) as Valor,\nsum(Frete) as Frete,\nsum(valor_com_frete) as Valor_Total,\ncount(distinct idcompra) as pedidos,\ncount(Item_Carrinho) itens,\ncount(distinct idcompraentrega) as ENVIOS,\nsum(Devolucao) as Devolucoes,\ncount(distinct (case when Flag_Atraso = 1 THEN idcompraentrega ELSE null END)) as atrasos,\nSUM(case when Flag_Atraso = 1 THEN (Tempo_atraso*(-1)) ELSE 0 END)/ count(distinct (case when Flag_Atraso = 1 THEN idcompraentrega ELSE null END)) as media_atraso,\nSUM(case when UPPER(status) = 'ENT' THEN 1 ELSE 0 END) as itens_entregues,\nSUM(case when UPPER(status) like '%CAN%' THEN 1 ELSE 0 END) as itens_cancelados,\nSUM(case when UPPER(status) not like '%CAN%' and UPPER(status) != 'ENT' THEN 1 ELSE 0 END) as itens_fluxo\n\nfrom carrinho\ngroup by 1,2,3,4,5,6,7,8,9,10) T1\n\nLEFT JOIN dia_nps nps\non T1.ano = nps.ano and T1.mes = nps.mes and T1.dia = nps.dia and T1.idlojista = nps.idlojista and  T1.idbandeira = nps.Bandeira\n''')", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "compilado_vendas.toPandas().to_csv('/home/spark/shared/tmp/compilado_vendas.csv', index=False)\nupload_file_cos(cgsClient, path,credentials_1,'compilado_vendas.csv')", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "compilado_vendas.csv - File Uploaded -30/05/2021 22:36:05\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "compilado_vendas.toPandas().to_json('/home/spark/shared/tmp/compilado_vendas.json',orient=\"records\")\nupload_file_cos(cgsClient, path,credentials_1,'compilado_vendas.json')", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "compilado_vendas.json - File Uploaded -30/05/2021 22:37:06\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# compilado_lojista = spark.sql('''\n# select\n# year(dia_pedido) as ano,\n# month(dia_pedido) as mes,\n# day(dia_pedido) as dia,\n# idlojista,\n# IDBANDEIRA,\n# sum(Valor) as Valor,\n# sum(Frete) as Frete,\n# sum(valor_com_frete) as Valor_Total,\n# count(distinct idcompra) as pedidos,\n# count(Item_Carrinho) itens,\n# count(distinct idcompraentrega) as ENVIOS,\n# count(distinct (case when Flag_Atraso = 1 THEN idcompraentrega ELSE null END)) as atrasos,\n# SUM(case when Flag_Atraso = 1 THEN (Tempo_atraso*(-1)) ELSE 0 END)/ count(distinct (case when Flag_Atraso = 1 THEN idcompraentrega ELSE null END)) as media_atraso,\n# SUM(case when UPPER(status) = 'ENT' THEN 1 ELSE 0 END) as itens_entregues,\n# SUM(case when UPPER(status) like '%CAN%' THEN 1 ELSE 0 END) as itens_cancelados,\n# SUM(case when UPPER(status) not like '%CAN%' and UPPER(status) != 'ENT' THEN 1 ELSE 0 END) as itens_fluxo\n\n# from carrinho\n# group by 1,2,3,4,5\n# ''')", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# top_sku = spark.sql('''\n# select\n# idlojista,\n# IDBANDEIRA,\n# idsku,\n# IDCATEGORIA,\n# CATEGORIA,\n# IDDEPARTAMENTO,\n# DEPARTAMENTO,\n# sum(Valor) as Valor,\n# sum(Frete) as Frete,\n# sum(Valor_Total) as Valor_Total,\n# sum(pedidos) as pedidos,\n# sum(itens) itens,\n# sum(ENVIOS) as ENVIOS,\n# sum(atrasos) as atrasos,\n# SUM(media_atraso)/sum(atrasos) as media_atraso,\n# SUM(itens_entregues) as itens_entregues,\n# SUM(itens_cancelados) as itens_cancelados,\n# SUM(itens_fluxo) as itens_fluxo\n\n# from vendas\n# group by 1,2,3,4,5,6,7\n# ''')", "execution_count": 22, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# top_categoria = spark.sql('''\n# select\n# idlojista,\n# IDBANDEIRA,\n# IDCATEGORIA,\n# CATEGORIA,\n# sum(Valor) as Valor,\n# sum(Frete) as Frete,\n# sum(Valor_Total) as Valor_Total,\n# sum(pedidos) as pedidos,\n# sum(itens) itens,\n# sum(ENVIOS) as ENVIOS,\n# sum(atrasos) as atrasos,\n# SUM(media_atraso)/sum(atrasos) as media_atraso,\n# SUM(itens_entregues) as itens_entregues,\n# SUM(itens_cancelados) as itens_cancelados,\n# SUM(itens_fluxo) as itens_fluxo\n\n# from vendas\n# group by 1,2,3,4\n# ''')", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# top_departamento = spark.sql('''\n# select\n# idlojista,\n# IDBANDEIRA,\n# IDDEPARTAMENTO,\n# DEPARTAMENTO,\n# sum(Valor) as Valor,\n# sum(Frete) as Frete,\n# sum(Valor_Total) as Valor_Total,\n# sum(pedidos) as pedidos,\n# sum(itens) itens,\n# sum(ENVIOS) as ENVIOS,\n# sum(atrasos) as atrasos,\n# SUM(media_atraso)/sum(atrasos) as media_atraso,\n# SUM(itens_entregues) as itens_entregues,\n# SUM(itens_cancelados) as itens_cancelados,\n# SUM(itens_fluxo) as itens_fluxo\n\n# from vendas\n# group by 1,2,3,4\n# order by 9 desc\n# ''')", "execution_count": 24, "outputs": []}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.7.10", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}